dat.filtered <- filter(dat, city == "Leipzig")
install.packages("dplyr")
dat <- read.csv("../data/European urban population, 700 - 2000.csv",
header=TRUE, encoding = "UTF-8")
dat <- read.csv("Studium/Digital Humanities/Uebung/data/European urban population, 700 - 2000.csv",
header=TRUE, encoding = "UTF-8")
library(dplyr)
dat.filtered <- filter(dat, city == "Leipzig")
dat <- as_tibble(dat)
dat.filtered <- filter(dat, city == "Leipzig")
dat <- read.csv("European urban population, 700 - 2000.csv",
header=TRUE, encoding = "UTF-8")
dat <- read.csv("Studium/Digital Humanities/Uebung/European urban population, 700 - 2000.csv",
header=TRUE, encoding = "UTF-8")
dat <- read.csv("Studium/Digital Humanities/Uebung/data/European urban population, 700 - 2000.csv",
header=TRUE, encoding = "UTF-8")
dat.filtered <- filter(dat, city == "Leipzig")
dat.filtered <- filter(dat, city == "Leipzig")
dat <- readRDS("Studium/Digital Humanities/Uebung/data/European urban population, 700 - 2000.csv")
dat <- as_tibble(dat)
subset(dat, city == "Leipzig")
subset(dat, city == "Leipzig" & year > 1900)
subset(dat, city == "Leipzig" & year > 1900,
select = c("inhabitants.in.000.s",
"year",
"city",
"synonyms.and.historical.names"))
dat[c(2,4),]
dat[c(2,4),c("year", "city")]
head(dat$city == "Leipzig")
length(dat$city == "Leipzig")
nrow(dat)
dat[dat$city == "Leipzig","city"] <- "Leipzig (Elster)"
!head(dat$city == "Leipzig")
dat[-(3:nrow(dat)),]
length(dat$city %in% "Leipzig")
length("Leipzig" %in% dat$city)
"Leipzig" %in% dat$city
any(is.na(dat$city))
#########################################################################
dat.filtered <- filter(dat, city == "Leipzig")
dat.filtered <- filter(dat, city == "Leipzig")
install.packages("dplyr")
dat.filtered <- filter(dat, city == "Leipzig")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("plotly")
install.packages("igraph")
install.packages("lda")
install.packages("LDAvis")
install.packages("LiblineaR")
install.packages("Matrix")
install.packages("quanteda")
install.packages("NLP")
install.packages("openNLP")
setwd("R")
getwd()
options(stringsAsFactors = FALSE)
library(quanteda)
require(topicmodels)
textdata <- readLines("C:/Users7sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt", sep = ",", encoding = "UTF-8")
textdata <- readLines("C:/Users7sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt", sep = "[", encoding = "UTF-8")
textdata <- readLines(".../Datensaetze/kafka_prozess_1925.txt", sep = ",", encoding = "UTF-8")
kafka_prozess_1925 <- read.table("~/R/Datensaetze/kafka_prozess_1925.txt", quote="\"")
kafka_prozess_1925 <- read.table("~/R/Datensaetze/kafka_prozess_1925.txt", quote="\"")
text <- readLines(".../Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
text <- readLines(""C:/Users/sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt"", encoding = "UTF-8")
text <- readLines("C:/Users/sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
lemma_data <- readLines("C:/Users/sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
textdata <- readLines("C:/Users/sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
stopwords_extended <- readLines("../Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
stopwords_extended <- readLines("../Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
stopwords_extended <- readLines("C:/Users/sophi/OneDrive/Dokumente/R/Datensaetze/kafka_prozess_1925.txt", encoding = "UTF-8")
prozess_corpus <- corpus(textdata$Paragraph, docnames = textdata$X)
prozess_corpus <- corpus(textdata$Paragraph, docnames = textdata$X)
prozess_corpus <- corpus(textdata$Paragraph, docnames = textdata$X)
prozess_corpus <- corpus(VectorSource(textdata))
prozess_corpus <- corpus(VectorSource(textdata))
prozess_corpus <- corpus(VectorSource(textdata))
library(tm)
prozess_corpus <- VCorpus(VectorSource(textdata))
corpus_tokens <- prozess_corpus %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed") %>%
tokens_remove(pattern = stopwords_extended, padding = T)
prozess_corpus <- corpus(VectorSource(textdata))
prozess_corpus <- VCorpus(VectorSource(textdata))
VCorpus_tokens <- prozess_corpus %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed") %>%
tokens_remove(pattern = stopwords_extended, padding = T)
VCorpus_tokens <- prozess_corpus %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed") %>%
tokens_remove(pattern = stopwords_extended, padding = T)
